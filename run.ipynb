{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Become an AI-powered AI Influencer\n",
    "\n",
    "In this Studio, we will explore how to create an AI-powered bot that will post commentary on AI-related content to social media "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find some content to talk about\n",
    "\n",
    "First, we'll need to find some trending AI-related content.  For this, we'll grab some content from [Hacker News](https://news.ycombinator.com/), a great source of curated content from the community.  We'll use their RSS feed and pluck the top entry with an AI-related keyword in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/\n"
     ]
    }
   ],
   "source": [
    "import scraper\n",
    "\n",
    "pageURL = scraper.findArticle()\n",
    "print(pageURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the content we want to talk about\n",
    "\n",
    "Next, we will use Selenium and Chrome to parse out the content from the webpage.  We want the title, a summary and the body content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Metaâ€™s GenAI Infrastructure - Engineering at Meta\n",
      "\n",
      "Search this site\n",
      "POSTED ON MARCH 12, 2024 TO AI RESEARCH, DATA CENTER ENGINEERING, ML APPLICATIONS\n",
      "Building Metaâ€™s GenAI Infrastructure\n",
      "By Kevin Lee, Adi Gangidi, Mathew Oldham\n",
      "Marking a major investment in Metaâ€™s AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training.\n",
      "We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry.\n",
      "This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, weâ€™re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.\n",
      "To lead in developing AI means leading investments in hardware infrastructure. Hardware infrastructure plays an important role in AIâ€™s future. Today, weâ€™re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta. These clusters support our current and next generation AI models, including Llama 3, the successor to Llama 2, our publicly released LLM, as well as AI research and development across GenAI and other areas .\n",
      "A peek into Metaâ€™s large-scale AI clusters\n",
      "Metaâ€™s long-term vision is to build artificial general intelligence (AGI) that is open and built responsibly so that it can be widely available for everyone to benefit from. As we work towards AGI, we have also worked on scaling our clusters to power this ambition. The progress we make towards AGI creates new products, new AI features for our family of apps, and new AI-centric computing devices. \n",
      "While weâ€™ve had a long history of building AI infrastructure, we first shared details on our AI Research SuperCluster (RSC), featuring 16,000 NVIDIA A100 GPUs, in 2022. RSC has accelerated our open and responsible AI research by helping us build our first generation of advanced AI models. It played and continues to play an important role in the development of Llama and Llama 2, as well as advanced AI models for applications ranging from computer vision, NLP, and speech recognition, to image generation, and even coding.\n",
      "Under the hood\n",
      "Our newer AI clusters build upon the successes and lessons learned from RSC. We focused on building end-to-end AI systems with a major emphasis on researcher and developer experience and productivity. The efficiency of the high-performance network fabrics within these clusters, some of the key storage decisions, combined with the 24,576 NVIDIA Tensor Core H100 GPUs in each, allow both cluster versions to support models larger and more complex than that could be supported in the RSC and pave the way for advancements in GenAI product development and AI research.\n",
      "Network\n",
      "At Meta, we handle hundreds of trillions of AI model executions per day. Delivering these services at a large scale requires a highly advanced and flexible infrastructure. Custom designing much of our own hardware, software, and network fabrics allows us to optimize the end-to-end experience for our AI researchers while ensuring our data centers operate efficiently. \n",
      "With this in mind, we built one cluster with a remote direct memory access (RDMA) over converged Ethernet (RoCE) network fabric solution based on the Arista 7800 with Wedge400 and Minipack2 OCP rack switches. The other cluster features an NVIDIA Quantum2 InfiniBand fabric. Both of these solutions interconnect 400 Gbps endpoints. With these two, we are able to assess the suitability and scalability of these different types of interconnect for large-scale training, giving us more insights that will help inform how we design and build even larger, scaled-up clusters in the future. Through careful co-design of the network, software, and model architectures, we have successfully used both RoCE and InfiniBand clusters for large, GenAI workloads (including our ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks.\n",
      "Compute\n",
      "Both clusters are built using Grand Teton, our in-house-designed, open GPU hardware platform that weâ€™ve contributed to the Open Compute Project (OCP). Grand Teton builds on the many generations of AI systems that integrate power, control, compute, and fabric interfaces into a single chassis for better overall performance, signal integrity, and thermal performance. It provides rapid scalability and flexibility in a simplified design, allowing it to be quickly deployed into data center fleets and easily maintained and scaled. Combined with other in-house innovations like our Open Rack power and rack architecture, Grand Teton allows us to build new clusters in a way that is purpose-built for current and future applications at Meta.\n",
      "We have been openly designing our GPU hardware platforms beginning with our Big Sur platform in 2015.\n",
      "Storage\n",
      "Storage plays an important role in AI training, and yet is one of the least talked-about aspects. As the GenAI training jobs become more multimodal over time, consuming large amounts of image, video, and text data, the need for data storage grows rapidly. The need to fit all that data storage into a performant, yet power-efficient footprint doesnâ€™t go away though, which makes the problem more interesting.\n",
      "Our storage deployment addresses the data and checkpointing needs of the AI clusters via a home-grown Linux Filesystem in Userspace (FUSE) API backed by a version of Metaâ€™s â€˜Tectonicâ€™ distributed storage solution optimized for Flash media. This solution enables thousands of GPUs to save and load checkpoints in a synchronized fashion (a challenge for any storage solution) while also providing a flexible and high-throughput exabyte scale storage required for data loading.\n",
      "We have also partnered with Hammerspace to co-develop and land a parallel network file system (NFS) deployment to meet the developer experience requirements for this AI cluster. Among other benefits, Hammerspace enables engineers to perform interactive debugging for jobs using thousands of GPUs as code changes are immediately accessible to all nodes within the environment. When paired together, the combination of our Tectonic distributed storage solution and Hammerspace enable fast iteration velocity without compromising on scale.     \n",
      "The storage deployments in our GenAI clusters, both Tectonic- and Hammerspace-backed, are based on the YV3 Sierra Point server platform, upgraded with the latest high capacity E1.S SSD we can procure in the market today. Aside from the higher SSD capacity, the servers per rack was customized to achieve the right balance of throughput capacity per server, rack count reduction, and associated power efficiency. Utilizing the OCP servers as Lego-like building blocks, our storage layer is able to flexibly scale to future requirements i\n"
     ]
    }
   ],
   "source": [
    "import scraper\n",
    "\n",
    "title, description, content = scraper.fetchPage(pageURL)\n",
    "print(title)\n",
    "print(description)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed this into our LLM\n",
    "\n",
    "Next, take the content and construct a prompt to instruct the LLM to create the content of the post for social media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting news from Meta as they announce the building of two 24k GPU clusters, marking a significant investment in their AI future ðŸš€. These clusters, designed for large-scale AI workloads and used for training Llama 3, will play a crucial role in developing advanced AI models and features for their apps ðŸ¤–. #AI #ML #DataCenterEngineering\n"
     ]
    }
   ],
   "source": [
    "import client\n",
    "\n",
    "prompt = f\"The content of an article is {content}\"\n",
    "prompt += \"\"\"Write a LinkedIn post post about a key point of the article's contents.\n",
    "You are not the author of this content.  Do not take credit.\n",
    "Use a professinal, but casual tone.\n",
    "The entire post should be two sentences.  Use a couple emojis too.  \n",
    "Respond with only the post, no additional commentary, no notes, no link.\"\"\"\n",
    "\n",
    "postContent = str(client.chat(\"mixtral\", prompt)).strip()\n",
    "print(postContent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with LinkedIn\n",
    "\n",
    "### Setup Authentication\n",
    "\n",
    "Create a [LinkedIn App](https://www.linkedin.com/developers/apps/new) and use the [token generator tool](https://www.linkedin.com/developers/tools/oauth/token-generator) to get a token.\n",
    "\n",
    "Save this token in your [teamspace settings](https://lightning.ai/docs/overview/Studios/secrets) or declare in `./lightning-studio/.studiorc`\n",
    "\n",
    "### Post to the Network\n",
    "\n",
    "The following functions from `social.py` will take the content generated above and share it to LinkedIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import social\n",
    "\n",
    "social.postLinkedIn(postContent, pageURL, title, description)\n",
    "social.postTwitter(postContent, pageURL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
